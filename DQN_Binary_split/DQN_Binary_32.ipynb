{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f18892aa-bae8-44ce-a5d6-fa9a43a3fa29",
   "metadata": {},
   "source": [
    "# DQN (Roy Debugging Playground)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bc91a3-d715-4055-ab50-d30575a1eea2",
   "metadata": {},
   "source": [
    "# Import the Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e75cec14-5597-482a-94dd-4f88fc4275b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "from PortfolioRebalancerEnv_Roy import PortfolioRebalancerEnv\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "import plotly.graph_objs as go\n",
    "import plotly.subplots as sp\n",
    "import plotly.express as px\n",
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cef71d8-fac2-425c-be4f-493f6e8762f6",
   "metadata": {},
   "source": [
    "# Create Replay Memory Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb188a5c-ce25-42df-9fb6-874e25c0d766",
   "metadata": {},
   "outputs": [],
   "source": [
    "Transition = namedtuple('Transition',\n",
    "                        ('state', 'action', 'next_state', 'reward'))\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "\n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        \"\"\"Save a transition\"\"\"\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f04ae8-b93e-48cb-a842-eeb8b0d95016",
   "metadata": {},
   "source": [
    "# Create Q-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51b2bcb0-f61e-497a-9827-544ea21267a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, n_observations, n_actions, name, tc, chkpt_dir='results/models'):\n",
    "        super(DQN, self).__init__()\n",
    "        self.layer1 = nn.Linear(n_observations, 32)\n",
    "        self.layer2 = nn.Linear(32, 32)\n",
    "        self.layer3 = nn.Linear(32, n_actions)\n",
    "\n",
    "        self.checkpoint_file = os.path.join(chkpt_dir, name + f'_tc_{tc}_assets_{n_observations}_dqn')\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer1(x))\n",
    "        x = F.relu(self.layer2(x))\n",
    "        return self.layer3(x)\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        print('... saving checkpoint ...')\n",
    "        torch.save(self.state_dict(), self.checkpoint_file)\n",
    "\n",
    "    def load_checkpoint(self):\n",
    "        print('... loading checkpoint ...')\n",
    "        self.load_state_dict(torch.load(self.checkpoint_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73f10c3f-d5a2-4b03-bd8e-622f5cdf9244",
   "metadata": {},
   "outputs": [],
   "source": [
    "def net_sharpe(w1, mu, cov, w0, tc):\n",
    "    return (w1.dot(mu) - cost_turnover(w0, w1, tc)) / np.sqrt(w1.dot(cov).dot(w1.T))\n",
    "\n",
    "\n",
    "def obj_func(x, mu, cov):\n",
    "    return -x.dot(mu) / np.sqrt(x.dot(cov).dot(x))\n",
    "\n",
    "def find_optimal_wgt(mu, cov):\n",
    "\n",
    "    n = len(mu)\n",
    "    w_min = np.zeros(n)\n",
    "    w_max = np.ones(n) * 2 / n\n",
    "    x0 = np.ones(n) / n\n",
    "    bounds = np.vstack([w_min, w_max]).T\n",
    "\n",
    "    cstr = [{\"type\": \"eq\", \"fun\": lambda x: np.sum(x) - 1, \"jac\": lambda x: np.ones(n)}]\n",
    "    opt = minimize(fun=obj_func, x0=x0, args=(mu, cov),\n",
    "                   bounds=bounds,\n",
    "                   constraints=cstr,\n",
    "                   tol=1e-6,\n",
    "                   options={\"maxiter\": 10000})\n",
    "\n",
    "    if not opt.success:\n",
    "        raise ValueError(\"optimization failed: {}\".format(opt.message))\n",
    "\n",
    "    return opt.x / opt.x.sum()\n",
    "\n",
    "\n",
    "def cost_turnover(w0, w1, tc):\n",
    "    return np.sum(np.abs(w1 - w0) * tc) / 2\n",
    "\n",
    "\n",
    "def expected_cost_total(w0, w1, opt_w, mu, cov, tc):\n",
    "    opt_net_sharpe = net_sharpe(opt_w, mu, cov, w0, tc)\n",
    "    w1_net_sharpe = net_sharpe(w1, mu, cov, w0, tc)\n",
    "    return opt_net_sharpe - w1_net_sharpe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "656f84f0-1c90-4bed-a37e-8b472021ed9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_asset = 32\n",
    "mu = np.linspace(50, 200, n_asset) / 1e4\n",
    "sigma = np.linspace(300, 800, n_asset) / 1e4\n",
    "cov = np.diag(sigma ** 2)\n",
    "optimal_weight = find_optimal_wgt(mu, cov)\n",
    "x0 = np.ones(len(mu)) / len(mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8003c1e4-8d20-42a2-be8e-2695838febcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = PortfolioRebalancerEnv(\n",
    "    mu=mu,\n",
    "    sigma=cov,\n",
    "    w_optimal=optimal_weight,\n",
    "    n_assets=n_asset,\n",
    ")\n",
    "device = torch.device(\"cuda:0\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350b9f4b-8096-4be6-84ec-f8a4d2664620",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1235181-f2a5-4f99-81a7-9ada9b64ffe4",
   "metadata": {},
   "source": [
    "RL:\n",
    "- BATCH_SIZE is the number of transitions sampled from the replay buffer\n",
    "- GAMMA is the discount factor as mentioned in the previous section\n",
    "- EPS_START is the starting value of epsilon\n",
    "- EPS_END is the final value of epsilon\n",
    "- EPS_DECAY controls the rate of exponential decay of epsilon, higher means a slower decay\n",
    "- TAU is the update rate of the target network\n",
    "- LR is the learning rate of the AdamW optimizer\n",
    "\n",
    "Finance:\n",
    "- $\\mu$ is historical mean returns for each asset\n",
    "- $\\Sigma$ is the historical covariance of returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7fa4c218-77cc-42f5-938d-c4975b5cac2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RL:\n",
    "BATCH_SIZE = 256\n",
    "GAMMA = 0.9\n",
    "EPS_START = 0.9999\n",
    "EPS_END = 0.05\n",
    "EPS_DECAY = 1000\n",
    "TAU = 0.005\n",
    "LR = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a4350a25-44df-486b-81eb-64267ba1425a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get number of actions from gym action space\n",
    "n_actions = env.action_space.shape[0]\n",
    "# Get the number of state observations\n",
    "state, info = env.reset()\n",
    "n_observations = state.shape[1]\n",
    "\n",
    "policy_net = DQN(n_observations=n_observations, n_actions=n_actions, name=\"Policy\", tc=0).to(device)\n",
    "target_net = DQN(n_observations=n_observations, n_actions=n_actions, name=\"Target\", tc=0).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "memory = ReplayMemory(10000)\n",
    "\n",
    "steps_done = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d66738f-f196-46e2-b250-6206026b8c33",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def select_action(state):\n",
    "    global steps_done\n",
    "    sample = random.random()\n",
    "    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * steps_done / EPS_DECAY)\n",
    "    steps_done += 1\n",
    "    if sample > eps_threshold:\n",
    "        with torch.no_grad():\n",
    "            # t.max(1) will return the largest column value of each row.\n",
    "            # second column on max result is index of where max element was\n",
    "            # found, so we pick action with the larger expected reward.\n",
    "            # optimal_action_index = policy_net(state).max(1)[1].view(1, -1)\n",
    "            optimal_action_index = policy_net(state).argmax()\n",
    "            output = torch.tensor(env.action_space[optimal_action_index], device=device, dtype=torch.int64).unsqueeze(0)\n",
    "            return output\n",
    "    else:\n",
    "        temp_row_id = np.random.choice(env.action_space.shape[0], size=1)\n",
    "        output = torch.tensor(env.action_space[temp_row_id], device=device, dtype=torch.int64)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c1ea4e6-45de-4153-bef6-86753609b7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < BATCH_SIZE:\n",
    "        return\n",
    "    transitions = memory.sample(BATCH_SIZE)\n",
    "    # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n",
    "    # detailed explanation). This converts batch-array of Transitions\n",
    "    # to Transition of batch-arrays.\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    # print(batch.state)\n",
    "    # print(batch.action)\n",
    "    # print(batch.reward)\n",
    "    # print(\"training\")\n",
    "\n",
    "    # Compute a mask of non-final states and concatenate the batch elements\n",
    "    # (a final state would've been the one after which simulation ended)\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                            batch.next_state)), device=device, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                       if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    # int_act_batch = torch.tensor(np.array(list(map(convert_action_to_index, action_batch))), device=device, dtype=torch.int32)\n",
    "    # int_act_batch = action_batch.apply_(convert_action_to_index).to(torch.int64)\n",
    "\n",
    "    # int_act_batch=[]\n",
    "    # for i in action_batch:\n",
    "    #     int_act_batch.append(np.where(np.isclose(env.action_space, i).any(axis=1))[0].item())\n",
    "    #\n",
    "    # int_act_batch = torch.tensor(int_act_batch, dtype=torch.int64)[:, np.newaxis]\n",
    "    # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n",
    "    # columns of actions taken. These are the actions which would've been taken\n",
    "    # for each batch state according to policy_net\n",
    "    state_action_values = policy_net(state_batch).gather(1, action_batch.unsqueeze(1))\n",
    "\n",
    "    # Compute V(s_{t+1}) for all next states.\n",
    "    # Expected values of actions for non_final_next_states are computed based\n",
    "    # on the \"older\" target_net; selecting their best reward with max(1)[0].\n",
    "    # This is merged based on the mask, such that we'll have either the expected\n",
    "    # state value or 0 in case the state was final.\n",
    "    next_state_values = torch.zeros(BATCH_SIZE, device=device)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1)[0]\n",
    "    # Compute the expected Q values\n",
    "    expected_state_action_values = (next_state_values * GAMMA) + reward_batch\n",
    "\n",
    "    # Compute Huber loss\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    # print(f\"Loss = {loss.item()}\")\n",
    "\n",
    "    # Optimize the model\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b8a49e7b-92a7-419d-b961-6096d1725a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_models(pn, tn):\n",
    "    pn.save_checkpoint()\n",
    "    tn.save_checkpoint()\n",
    "\n",
    "def load_models(pn, tn):\n",
    "    pn.load_checkpoint()\n",
    "    tn.load_checkpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cc0fc6-f799-4916-b591-2c7cfae055c7",
   "metadata": {},
   "source": [
    "# Run Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a8a3306-10f3-462b-b964-c0bb815fa75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_episodes = 50_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4eed5a4-af70-4b66-98c7-b9d1a29018c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration Start\n",
      "Epoch 0 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 1000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 2000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 3000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 4000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 5000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 6000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 7000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 8000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 9000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 10000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 11000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 12000 | tc = 0 | Reward = -0.1908825784921646\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 13000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 14000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 15000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 16000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 17000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 18000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 19000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 20000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 21000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 22000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 23000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 24000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 25000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 26000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 27000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 28000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 29000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 30000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 31000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 32000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 33000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 34000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 35000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 36000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 37000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 38000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 39000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 40000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 41000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 42000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 43000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 44000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 45000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 46000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 47000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 48000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 49000 | tc = 0 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 0 | tc = 0.0005 | Reward = -0.1409701555967331\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 1000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 2000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 3000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 4000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 5000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 6000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 7000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 8000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 9000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 10000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 11000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 12000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 13000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 14000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 15000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 16000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 17000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 18000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 19000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 20000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 21000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 22000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 23000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 24000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 25000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 26000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 27000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 28000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 29000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 30000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 31000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 32000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 33000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 34000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 35000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 36000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 37000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 38000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 39000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 40000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 41000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 42000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 43000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 44000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 45000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 46000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 47000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 48000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 49000 | tc = 0.0005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 0 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 1000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 2000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 3000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 4000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 5000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 6000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 7000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 8000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 9000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 10000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 11000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 12000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 13000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 14000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 15000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 16000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 17000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 18000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 19000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 20000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 21000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 22000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 23000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 24000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 25000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 26000 | tc = 0.001 | Reward = -0.14289511740207672\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 27000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 28000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 29000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 30000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 31000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 32000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 33000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 34000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 35000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 36000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 37000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 38000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 39000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 40000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 41000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 42000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 43000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 44000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 45000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 46000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 47000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 48000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 49000 | tc = 0.001 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 0 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 1000 | tc = 0.002 | Reward = -0.15209108591079712\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 2000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 3000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 4000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 5000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 6000 | tc = 0.002 | Reward = -0.11918826401233673\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 7000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 8000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 9000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 10000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 11000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 12000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 13000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 14000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 15000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 16000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 17000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 18000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 19000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 20000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 21000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 22000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 23000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 24000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 25000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 26000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 27000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 28000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 29000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 30000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 31000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 32000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 33000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 34000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 35000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 36000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 37000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 38000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 39000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 40000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 41000 | tc = 0.002 | Reward = -0.08817199617624283\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 42000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 43000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 44000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 45000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 46000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 47000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 48000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 49000 | tc = 0.002 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 0 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 1000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 2000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 3000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 4000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 5000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 6000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 7000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 8000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 9000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 10000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 11000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 12000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 13000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 14000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 15000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 16000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 17000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 18000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 19000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 20000 | tc = 0.005 | Reward = -0.026621602475643158\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 21000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 22000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 23000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 24000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 25000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 26000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 27000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 28000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 29000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 30000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 31000 | tc = 0.005 | Reward = -0.04382294416427612\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 32000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 33000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 34000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 35000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 36000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 37000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 38000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 39000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 40000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 41000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 42000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 43000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 44000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 45000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 46000 | tc = 0.005 | Reward = -0.04001335799694061\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 47000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 48000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 49000 | tc = 0.005 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 0 | tc = 0.01 | Reward = 0.13343511521816254\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 1000 | tc = 0.01 | Reward = 0.12998497486114502\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 2000 | tc = 0.01 | Reward = 0.07703500986099243\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 3000 | tc = 0.01 | Reward = 0.10488760471343994\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 4000 | tc = 0.01 | Reward = 0.11230122298002243\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 5000 | tc = 0.01 | Reward = 0.11930826306343079\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 6000 | tc = 0.01 | Reward = 0.1104368194937706\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 7000 | tc = 0.01 | Reward = 0.12327396869659424\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 8000 | tc = 0.01 | Reward = 0.10870248824357986\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 9000 | tc = 0.01 | Reward = 0.12515562772750854\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 10000 | tc = 0.01 | Reward = 0.0683601051568985\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 11000 | tc = 0.01 | Reward = 0.09328761696815491\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 12000 | tc = 0.01 | Reward = 0.09105299413204193\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 13000 | tc = 0.01 | Reward = 0.10576791316270828\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 14000 | tc = 0.01 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 15000 | tc = 0.01 | Reward = 0.09088725596666336\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 16000 | tc = 0.01 | Reward = 0.09216141700744629\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 17000 | tc = 0.01 | Reward = 0.09988351911306381\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 18000 | tc = 0.01 | Reward = 0.09474379569292068\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 19000 | tc = 0.01 | Reward = 0.083185113966465\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 20000 | tc = 0.01 | Reward = 0.0681750476360321\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 21000 | tc = 0.01 | Reward = 0.10146120190620422\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 22000 | tc = 0.01 | Reward = 0.11633402109146118\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 23000 | tc = 0.01 | Reward = 0.07746586203575134\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 24000 | tc = 0.01 | Reward = 0.07554802298545837\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 25000 | tc = 0.01 | Reward = 0.08446166664361954\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 26000 | tc = 0.01 | Reward = 0.09569346159696579\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 27000 | tc = 0.01 | Reward = 0.14061908423900604\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 28000 | tc = 0.01 | Reward = 0.09340506792068481\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 29000 | tc = 0.01 | Reward = 0.09839913994073868\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 30000 | tc = 0.01 | Reward = 0.13145630061626434\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 31000 | tc = 0.01 | Reward = 0.09557314962148666\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 32000 | tc = 0.01 | Reward = 0.1410142332315445\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 33000 | tc = 0.01 | Reward = 0.1056329682469368\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 34000 | tc = 0.01 | Reward = 0.1162962093949318\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 35000 | tc = 0.01 | Reward = 0.08272392302751541\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 36000 | tc = 0.01 | Reward = 0.12707401812076569\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 37000 | tc = 0.01 | Reward = 0.06274881958961487\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 38000 | tc = 0.01 | Reward = 0.060871828347444534\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 39000 | tc = 0.01 | Reward = 0.07241031527519226\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 40000 | tc = 0.01 | Reward = 0.06677041202783585\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 41000 | tc = 0.01 | Reward = 0.10979663580656052\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 42000 | tc = 0.01 | Reward = 0.1025448888540268\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 43000 | tc = 0.01 | Reward = 0.09859848022460938\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 44000 | tc = 0.01 | Reward = 0.10300742089748383\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 45000 | tc = 0.01 | Reward = 0.09950654208660126\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 46000 | tc = 0.01 | Reward = 0.07003048807382584\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 47000 | tc = 0.01 | Reward = 0.09584098309278488\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 48000 | tc = 0.01 | Reward = 0.09442217648029327\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 49000 | tc = 0.01 | Reward = 0.07985024899244308\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 0 | tc = 0.05 | Reward = 1.1496398448944092\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 1000 | tc = 0.05 | Reward = -0.0\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 2000 | tc = 0.05 | Reward = 1.4431407451629639\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 3000 | tc = 0.05 | Reward = 1.6782234907150269\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 4000 | tc = 0.05 | Reward = 0.9240954518318176\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 5000 | tc = 0.05 | Reward = 0.933899998664856\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 6000 | tc = 0.05 | Reward = 1.2496192455291748\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 7000 | tc = 0.05 | Reward = 1.4566060304641724\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 8000 | tc = 0.05 | Reward = 1.4680086374282837\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 9000 | tc = 0.05 | Reward = 1.1680635213851929\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 10000 | tc = 0.05 | Reward = 1.441870093345642\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 11000 | tc = 0.05 | Reward = 1.2021442651748657\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 12000 | tc = 0.05 | Reward = 1.184949517250061\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 13000 | tc = 0.05 | Reward = 1.0347918272018433\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 14000 | tc = 0.05 | Reward = 1.2226536273956299\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 15000 | tc = 0.05 | Reward = 1.4294540882110596\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 16000 | tc = 0.05 | Reward = 1.350549340248108\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 17000 | tc = 0.05 | Reward = 1.2798354625701904\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 18000 | tc = 0.05 | Reward = 1.1814854145050049\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 19000 | tc = 0.05 | Reward = 1.5531699657440186\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 20000 | tc = 0.05 | Reward = 1.3661082983016968\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 21000 | tc = 0.05 | Reward = 1.221235990524292\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 22000 | tc = 0.05 | Reward = 1.1259500980377197\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 23000 | tc = 0.05 | Reward = 1.27359938621521\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 24000 | tc = 0.05 | Reward = 1.3533416986465454\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 25000 | tc = 0.05 | Reward = 1.2902312278747559\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 26000 | tc = 0.05 | Reward = 1.769972562789917\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 27000 | tc = 0.05 | Reward = 1.5062260627746582\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 28000 | tc = 0.05 | Reward = 1.1605236530303955\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 29000 | tc = 0.05 | Reward = 1.0574384927749634\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 30000 | tc = 0.05 | Reward = 1.1903326511383057\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 31000 | tc = 0.05 | Reward = 1.1685373783111572\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 32000 | tc = 0.05 | Reward = 1.1449309587478638\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 33000 | tc = 0.05 | Reward = 1.3096085786819458\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 34000 | tc = 0.05 | Reward = 1.0772550106048584\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 35000 | tc = 0.05 | Reward = 1.0184599161148071\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 36000 | tc = 0.05 | Reward = 1.5183260440826416\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 37000 | tc = 0.05 | Reward = 1.3271455764770508\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 38000 | tc = 0.05 | Reward = 1.3476617336273193\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 39000 | tc = 0.05 | Reward = 1.2174947261810303\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 40000 | tc = 0.05 | Reward = 1.4554003477096558\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 41000 | tc = 0.05 | Reward = 1.1075471639633179\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 42000 | tc = 0.05 | Reward = 1.0051612854003906\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 43000 | tc = 0.05 | Reward = 1.1907511949539185\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 44000 | tc = 0.05 | Reward = 1.214246392250061\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 45000 | tc = 0.05 | Reward = 1.0880887508392334\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 46000 | tc = 0.05 | Reward = 1.1486146450042725\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 47000 | tc = 0.05 | Reward = 1.1446853876113892\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 48000 | tc = 0.05 | Reward = 1.2029227018356323\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "Epoch 49000 | tc = 0.05 | Reward = 1.119543433189392\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n"
     ]
    }
   ],
   "source": [
    "weightA_array, weightA_dict = [], {}\n",
    "action_array, action_dict = [], {}\n",
    "reward_array, reward_dict = [], {}\n",
    "\n",
    "policy_net_dict = {}\n",
    "target_net_dict = {}\n",
    "\n",
    "print(\"Iteration Start\")\n",
    "\n",
    "dqn_result = {}\n",
    "dqn_loss = {}\n",
    "tc_list = [0, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.05]\n",
    "\n",
    "for tc in tc_list:\n",
    "\n",
    "    # Get number of actions from gym action space\n",
    "    n_actions = env.action_space.shape[0]\n",
    "    # Get the number of state observations\n",
    "    state, info = env.reset()\n",
    "\n",
    "    # reset env with new transaction cost\n",
    "    env.transaction_costs = tc\n",
    "\n",
    "    n_observations = state.shape[1]\n",
    "\n",
    "    policy_net = DQN(n_observations=n_observations, n_actions=n_actions, name=\"Policy\", tc=tc).to(device)\n",
    "    target_net = DQN(n_observations=n_observations, n_actions=n_actions, name=\"Target\", tc=tc).to(device)\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "    optimizer = optim.AdamW(policy_net.parameters(), lr=LR, amsgrad=True)\n",
    "    memory = ReplayMemory(10000)\n",
    "    steps_done = 0\n",
    "\n",
    "    for i_episode in range(num_episodes):\n",
    "        # print(\"Epoch {}\".format(i_episode))\n",
    "\n",
    "        # Initialize the environment and get it's state\n",
    "        state, info = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=device)\n",
    "        # w_A_dict[i].append(state.squeeze().clone().detach())\n",
    "\n",
    "        action = select_action(state)\n",
    "        observation, reward, done = env.step(action.cpu().numpy().ravel())\n",
    "        reward = torch.tensor(reward, device=device, dtype=torch.float32)\n",
    "\n",
    "        if done:\n",
    "            next_state = None\n",
    "            # store variables for plotting\n",
    "            weightA_array.append(state.squeeze().clone().detach())\n",
    "            reward_array.append(reward.squeeze().clone().detach())\n",
    "            action_array.append(action.squeeze().clone().detach())\n",
    "        else:\n",
    "            # get new state\n",
    "            next_state = torch.tensor(observation, dtype=torch.float32, device=device)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        memory.push(state, action, next_state, reward)\n",
    "\n",
    "        # Move to the next state\n",
    "        state = next_state\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        optimize_model()\n",
    "\n",
    "        # Soft update of the target network's weights\n",
    "        # θ′ ← τ θ + (1 −τ )θ′\n",
    "        target_net_state_dict = target_net.state_dict()\n",
    "        policy_net_state_dict = policy_net.state_dict()\n",
    "        for key in policy_net_state_dict:\n",
    "            target_net_state_dict[key] = policy_net_state_dict[key]*TAU + target_net_state_dict[key]*(1-TAU)\n",
    "        target_net.load_state_dict(target_net_state_dict)\n",
    "\n",
    "        \n",
    "        if i_episode % 1000 == 0:\n",
    "            print(f\"Epoch {i_episode} | tc = {tc} | Reward = {reward.squeeze().clone().detach()}\")\n",
    "            save_models(pn=policy_net, tn=target_net)\n",
    "\n",
    "    # store tc results\n",
    "    dqn_result[tc] = env\n",
    "\n",
    "    weightA_dict[tc] = weightA_array\n",
    "    action_dict[tc] = action_array\n",
    "    reward_dict[tc] = reward_array\n",
    "    policy_net_dict[tc] = policy_net\n",
    "    target_net_dict[tc] = target_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4195026f-d8ba-4529-9241-6eec7c1574c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(1, 101)\n",
    "state_possible = np.array(np.meshgrid(*([x] * 2))).T.reshape(-1, 2).astype(\n",
    "    np.float32)\n",
    "state_possible = state_possible[state_possible.sum(axis=1) == 100, :] / 100\n",
    "tc_list = [0, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a851899-a152-42cc-9cf0-7991b4d9a13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = np.arange(1, 101)\n",
    "# spn = np.array(np.meshgrid(*([x] * len(mu)))).T.reshape(-1, len(mu)).astype(\n",
    "#     np.float32)\n",
    "# spn = spn[spn.sum(axis=1) == 100, :] / 100\n",
    "# tc_list = [0, 0.0005, 0.001, 0.002, 0.005, 0.01, 0.05]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "24481d59-f013-4d9c-b6f3-d7b92e45c315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = state_possible[:, 0]\n",
    "# dqn_action_df = pd.DataFrame(index=x)\n",
    "# for tc, mo in dqn_result.items():\n",
    "#     # visualize q table\n",
    "#     action = []\n",
    "#     for j in range(x.shape[0]):\n",
    "#         qval = policy_net_dict[tc](torch.FloatTensor(state_possible[j])).detach().numpy()\n",
    "#         action.append(mo.action_space[qval.argmax(), 0])\n",
    "#\n",
    "#     action = np.array(action)\n",
    "#     dqn_action_df[f\"TC: {tc * 1e4:.0f} bps\"] = action\n",
    "#\n",
    "# dqn_action_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "23a79bd2-7483-414d-b1e2-b520526a3fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tmp_device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfff4576-32ac-4964-945a-a7b5bfd9ee3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = state_possible[:, 0]\n",
    "# dqn_action_df = pd.DataFrame(index=x)\n",
    "# for tc in tc_list:\n",
    "#     a = DQN(n_observations=n_observations, n_actions=n_actions, name=\"Policy\", tc=tc).to(tmp_device)\n",
    "#     b = DQN(n_observations=n_observations, n_actions=n_actions, name=\"Policy\", tc=tc).to(tmp_device)\n",
    "\n",
    "#     load_models(pn=a, tn=b)\n",
    "\n",
    "#     # visualize q table\n",
    "#     action = []\n",
    "#     for j in range(x.shape[0]):\n",
    "#         qval = a(torch.FloatTensor(spn[j])).detach().numpy()\n",
    "#         action.append(env.action_space[qval.argmax()])\n",
    "\n",
    "#     action = np.array(action)\n",
    "#     dqn_action_df[f\"TC: {tc * 1e4:.0f} bps\"] = action\n",
    "\n",
    "# dqn_action_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1323170f-ca96-47fb-8adf-725e12b790d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%\n",
    "# x = state_possible[:, 0]\n",
    "# dqn_action_df = pd.DataFrame(index=x)\n",
    "# for tc, mo in dqn_result.items():\n",
    "#     # visualize q table\n",
    "#     action = []\n",
    "#     for j in range(x.shape[0]):\n",
    "#         qval = policy_net_dict[tc](torch.FloatTensor(state_possible[j])).detach().numpy()\n",
    "#         action.append(mo.action_space[qval.argmax()])\n",
    "#\n",
    "#     action = np.array(action)\n",
    "#     dqn_action_df[f\"TC: {tc * 1e4:.0f} bps\"] = action\n",
    "#\n",
    "# dqn_action_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ca07bd1e-b164-41bf-b2d0-308d1f744273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dqn_action_df.to_csv(\"dqn_action_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "584758fd-35ae-4319-8e99-3a57200a2d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.rcParams.update({\"font.size\": 18})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "68778e82-0315-4ef8-aad8-21b01bcf8e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
    "# # ql_action_df.plot(ax=ax, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "# # bell_action_df.plot(ax=ax, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'], linestyle=\"dashed\")\n",
    "# dqn_action_df.plot(ax=ax, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'], linestyle=\"dotted\")\n",
    "# ax.set_xlabel(\"Weight on First Asset\")\n",
    "# ax.set_ylabel(\"Suggested Delta Weight on Asset 1\")\n",
    "# ax.axvline(optimal_weight[0], color=\"red\", linestyle=\"dotted\")\n",
    "# ax.axhline(0, color=\"red\", linestyle=\"dotted\")\n",
    "# ax.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "18082637-c8ea-495d-a939-086d4fdec249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fix_reward(x):\n",
    "#     return [i.cpu().item() for i in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46639f18-a16f-4991-8070-99d1c43ff626",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward_df = pd.DataFrame()\n",
    "# for col, val in reward_dict.items():\n",
    "#     reward_df[col] = fix_reward(reward_dict[col])\n",
    "\n",
    "# reward_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b9d2c70c-0e52-43ef-bd45-e94c220df0be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.heatmap(dqn_action_df, cmap='cividis')\n",
    "\n",
    "# y = optimal_weight[0]\n",
    "# ax = plt.gca()\n",
    "# plt.hlines(y=y*100, xmin=0, xmax=len(dqn_action_df.columns), colors='red')\n",
    "# plt.title(\"DQN Binary Heatmap of Rebalancing\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5d62f3-9d17-4dd4-8b65-92ae9a2d70cd",
   "metadata": {},
   "source": [
    "# DP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "059a32b1-5cdf-4a13-a886-c3b74829b3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# og_path = '/Users/Roy/Desktop/College/GeorgiaTech/Research/Portfolio-Rebalancing-DQN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4f4b32b8-e87c-4f63-b6f8-1217c28f2f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dp_path = og_path + '/DP_Binary/results/models/'\n",
    "# os.chdir(dp_path)\n",
    "# #%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "67f51d07-f158-478a-af5c-9cf3bd66d34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #%%\n",
    "# df_0 = pd.read_csv(\"dp_tc_0_assets_2\", index_col=0, dtype=np.float32)\n",
    "# df_01 = pd.read_csv(\"dp_tc_0.001_assets_2\", index_col=0, dtype=np.float32)\n",
    "# df_02 = pd.read_csv(\"dp_tc_0.002_assets_2\", index_col=0, dtype=np.float32)\n",
    "# df_05 = pd.read_csv(\"dp_tc_0.0005_assets_2\", index_col=0, dtype=np.float32)\n",
    "# df_5 = pd.read_csv(\"dp_tc_0.005_assets_2\", index_col=0, dtype=np.float32)\n",
    "# df_1 = pd.read_csv(\"dp_tc_0.01_assets_2\", index_col=0, dtype=np.float32)\n",
    "# df_5 = pd.read_csv(\"dp_tc_0.05_assets_2\", index_col=0, dtype=np.float32)\n",
    "# #%%\n",
    "# df_list = [df_0, df_05, df_01, df_02, df_5, df_1, df_5]\n",
    "# #%%\n",
    "# x = pd.to_numeric(df_0.index.values)\n",
    "# action_possible = pd.to_numeric(df_0.columns.values)\n",
    "# #%%\n",
    "# action_df = pd.DataFrame(index=x)\n",
    "# action_bm_df = pd.DataFrame(index=x)\n",
    "# for tc, bell in zip(tc_list, df_list):\n",
    "#     # visualize q table\n",
    "#     action = np.array([action_possible[i] for i in np.argmax(bell, axis=1)])\n",
    "#     action_df[f\"TC: {tc * 1e4:.0f} bps BM\"] = action\n",
    "#     action_bm = []\n",
    "#     for i in x:\n",
    "#         a = x[np.argmax([net_sharpe(np.array([j, 1 - j]), mu, cov, np.array([i, 1 - i]), tc) for j in x])] - i\n",
    "#         action_bm.append(a)\n",
    "#     action_bm = np.array(action_bm)\n",
    "#     action_bm_df[f\"TC: {tc * 1e4:.0f} bps BM (GT)\"] = action_bm\n",
    "# #%%\n",
    "# optimal_weight = find_optimal_wgt(mu, cov)\n",
    "# optimal_weight\n",
    "# #%%\n",
    "# fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
    "# action_df.plot(ax=ax, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "# action_bm_df.plot(ax=ax, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'], linestyle=\"dashed\", alpha=0.5)\n",
    "# ax.set_xlabel(\"Weight on First Asset\")\n",
    "# ax.set_ylabel(\"Suggested Delta Weight on Asset 1\")\n",
    "# ax.axvline(optimal_weight[0], color=\"red\", linestyle=\"dotted\")\n",
    "# ax.axhline(0, color=\"red\", linestyle=\"dotted\")\n",
    "# ax.legend()\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "# plt.close()\n",
    "\n",
    "# #%%\n",
    "# # Create subplots\n",
    "# fig = sp.make_subplots(rows=1, cols=1, subplot_titles=\"Comparison of BM and Net Sharpe State Possible Outputs\", shared_yaxes=True)\n",
    "\n",
    "# # Add traces for Action DQN and Action BM\n",
    "# for col, color, linestyle in zip(action_df.columns, ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#2a8bde', '#ff6e51', '#4caf50'], ['solid']*len(action_df.columns)):\n",
    "#     trace_dqn = go.Scatter(x=action_df.index, y=action_df[col], mode='lines', name=f'Action BM - {col}', line=dict(color=color, dash=\"solid\"))\n",
    "#     fig.add_trace(trace_dqn, row=1, col=1)\n",
    "\n",
    "# # Add traces for Action DQN and Action BM\n",
    "# for col, color, linestyle in zip(action_bm_df.columns, ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#2a8bde', '#ff6e51', '#4caf50'], ['dot']*len(action_bm_df.columns)):\n",
    "#     trace_bm = go.Scatter(x=action_bm_df.index, y=action_bm_df[col], mode='lines', name=f'Ground Truth - {col}', line=dict(color=color, dash=linestyle))\n",
    "#     fig.add_trace(trace_bm, row=1, col=1)\n",
    "\n",
    "# # Add vertical and horizontal lines\n",
    "# fig.add_shape(dict(\n",
    "#     type=\"line\",\n",
    "#     x0=optimal_weight[0],\n",
    "#     x1=optimal_weight[0],\n",
    "#     y0=action_df.values.min(),\n",
    "#     y1=action_df.values.max(),\n",
    "#     line=dict(color=\"gold\", dash=\"dot\")\n",
    "# ))\n",
    "\n",
    "# fig.add_shape(dict(\n",
    "#     type=\"line\",\n",
    "#     x0=action_df.index.min(),\n",
    "#     x1=action_df.index.max(),\n",
    "#     y0=0,\n",
    "#     y1=0,\n",
    "#     line=dict(color=\"gold\", dash=\"dot\")\n",
    "# ))\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     title=\"Comparison of BM and Net Sharpe State Possible Outputs\",\n",
    "#     xaxis=dict(title=\"Weight on First Asset\"),\n",
    "#     yaxis=dict(title=\"Suggested Delta Weight on Asset 1\"),\n",
    "#     showlegend=True,\n",
    "# )\n",
    "\n",
    "# # Show the figure\n",
    "# fig.show()\n",
    "\n",
    "# #%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "982d63f4-c604-439c-8d21-233c22cc6d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming `dqn_bool_action_df`, `action_df`, and `optimal_weight` are available\n",
    "\n",
    "# # Create subplots\n",
    "# fig = sp.make_subplots(rows=1, cols=1, subplot_titles=(\"Comparison of DQN and Action\"), shared_yaxes=True)\n",
    "\n",
    "# # Add traces for DQN and Action\n",
    "# for col, color, linestyle in zip(dqn_action_df.columns, ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#2a8bde', '#ff6e51', '#4caf50'], ['solid']*len(dqn_action_df.columns)):\n",
    "#     trace_dqn = go.Scatter(x=dqn_action_df.index, y=dqn_action_df[col], mode='lines', name=f'DQN - {col}', line=dict(color=color, dash=linestyle))\n",
    "#     fig.add_trace(trace_dqn, row=1, col=1)\n",
    "\n",
    "# # Add traces for DQN and Action\n",
    "# for col, color, linestyle in zip(action_df.columns, ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#2a8bde', '#ff6e51', '#4caf50'], ['dash']*len(action_df.columns)):\n",
    "#     trace_action = go.Scatter(x=action_df.index, y=action_df[col], mode='lines', name=f'BM - {col}', line=dict(color=color, dash=linestyle))\n",
    "#     fig.add_trace(trace_action, row=1, col=1)\n",
    "\n",
    "# # Add traces for Action DQN and Action BM\n",
    "# for col, color, linestyle in zip(action_bm_df.columns, ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#2a8bde', '#ff6e51', '#4caf50'], ['dot']*len(action_bm_df.columns)):\n",
    "#     trace_bm = go.Scatter(x=action_bm_df.index, y=action_bm_df[col], mode='lines', name=f'Ground Truth - {col}', line=dict(color=color, dash=linestyle))\n",
    "#     fig.add_trace(trace_bm, row=1, col=1)\n",
    "\n",
    "# # Add vertical and horizontal lines\n",
    "# fig.add_shape(dict(\n",
    "#     type=\"line\",\n",
    "#     x0=optimal_weight[0],\n",
    "#     x1=optimal_weight[0],\n",
    "#     y0=action_df.values.min(),\n",
    "#     y1=action_df.values.max(),\n",
    "#     line=dict(color=\"gold\", dash=\"dot\")\n",
    "# ))\n",
    "\n",
    "# fig.add_shape(dict(\n",
    "#     type=\"line\",\n",
    "#     x0=dqn_action_df.index.min(),\n",
    "#     x1=dqn_action_df.index.max(),\n",
    "#     y0=0,\n",
    "#     y1=0,\n",
    "#     line=dict(color=\"gold\", dash=\"dot\")\n",
    "# ))\n",
    "\n",
    "# # Update layout\n",
    "# fig.update_layout(\n",
    "#     title=\"Comparison of DQN and Action\",\n",
    "#     xaxis=dict(title=\"Weight on First Asset\"),\n",
    "#     yaxis=dict(title=\"Suggested Delta Weight on Asset 1\"),\n",
    "#     showlegend=True,\n",
    "# )\n",
    "\n",
    "# # Show the figure\n",
    "# fig.show()\n",
    "\n",
    "# #%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "abfbae91-4e58-4727-9947-51d3e1717510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Assuming `dqn_bool_action_df` and `bool_action_bm_df` are your DataFrames\n",
    "# dqn_df = dqn_action_df.copy()\n",
    "# bm_df = action_df.copy()\n",
    "\n",
    "# # Create a heatmap trace for DQN Binary\n",
    "# trace_dqn = go.Heatmap(z=dqn_df, x=dqn_df.columns, y=dqn_df.index, name=\"DQN Binary\")\n",
    "\n",
    "# # Create a heatmap trace for DP Bellman\n",
    "# trace_dp = go.Heatmap(z=bm_df, x=bm_df.columns, y=bm_df.index, name=\"DP Bellman\")\n",
    "\n",
    "# # Create a heatmap trace for Q Table\n",
    "# # trace_q = go.Heatmap(z=q_df, x=q_df.columns, y=q_df.index, name=\"Q Table\")\n",
    "\n",
    "# # Create the layout with 'overlay' parameter set to \"overlay\"\n",
    "# layout = go.Layout(\n",
    "#     title=\"Comparison of DQN Binary, DP Bellman, and Q Learning\",\n",
    "#     xaxis=dict(title=\"Transaction Cost (bps)\",\n",
    "#                showgrid=True),\n",
    "#     yaxis=dict(title=\"Weight on First Asset\",\n",
    "#                showgrid=True))\n",
    "\n",
    "# # Add horizontal line at y=0.50\n",
    "# layout.shapes = [\n",
    "#     dict(\n",
    "#         type=\"line\",\n",
    "#         x0=dqn_df.columns[0],  # Adjust x0 to the starting point of your data\n",
    "#         x1=dqn_df.columns[-1],  # Adjust x1 to the ending point of your data\n",
    "#         y0=optimal_weight[0],\n",
    "#         y1=optimal_weight[0],\n",
    "#         line=dict(color=\"red\", width=2),  # You can customize the color and width of the line\n",
    "#     ),\n",
    "#     dict(\n",
    "#         type=\"line\",\n",
    "#         x0=bm_df.columns[0],  # Adjust x0 to the starting point of your data\n",
    "#         x1=bm_df.columns[-1],  # Adjust x1 to the ending point of your data\n",
    "#         y0=optimal_weight[0],\n",
    "#         y1=optimal_weight[0],\n",
    "#         line=dict(color=\"red\", width=2),  # You can customize the color and width of the line\n",
    "#     ),\n",
    "#     # dict(\n",
    "#     #     type=\"line\",\n",
    "#     #     x0=q_df.columns[0],  # Adjust x0 to the starting point of your data\n",
    "#     #     x1=q_df.columns[-1],  # Adjust x1 to the ending point of your data\n",
    "#     #     y0=optimal_weight[0],\n",
    "#     #     y1=optimal_weight[0],\n",
    "#     #     line=dict(color=\"red\", width=2))\n",
    "# ]\n",
    "\n",
    "# # Create the figure\n",
    "# fig = go.Figure(data=[trace_dp], layout=layout)\n",
    "# fig.show()\n",
    "\n",
    "# fig = go.Figure(data=[trace_dqn], layout=layout)\n",
    "# fig.show()\n",
    "\n",
    "# # fig = go.Figure(data=[trace_q], layout=layout)\n",
    "# # fig.show()\n",
    "\n",
    "# #%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "965a0443-2107-47d6-8096-172a60c711a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create an empty dataframe to store the confusion matrices\n",
    "# confusion_matrices = {}\n",
    "\n",
    "# # Iterate through the columns and calculate confusion matrices\n",
    "# for col1, col2 in zip(bm_df.columns, dqn_df.columns):\n",
    "#     true_labels = bm_df[col1]\n",
    "#     predicted_labels = dqn_df[col2]\n",
    "\n",
    "#     # Create the confusion matrix\n",
    "#     confusion = confusion_matrix(true_labels, predicted_labels)\n",
    "#     confusion_matrices[col1] = confusion\n",
    "\n",
    "# # Print or use the confusion matrices as needed\n",
    "# for column, confusion in confusion_matrices.items():\n",
    "#     print(f\"Confusion Matrix for {column}:\")\n",
    "#     print(confusion)\n",
    "\n",
    "# #%%\n",
    "# dqn_noreb = {}\n",
    "# bm_noreb = {}\n",
    "# for col1, col2 in zip(bm_df.columns, dqn_df.columns):\n",
    "#     bm_index = bm_df[bm_df[col1] == 0].index.values\n",
    "#     dqn_index = dqn_df[dqn_df[col2] == 0].index.values\n",
    "#     bm_noreb[col2] = bm_index\n",
    "#     dqn_noreb[col2] = dqn_index\n",
    "# #%%\n",
    "# # Create a DataFrame from the dictionary, excluding keys with empty values\n",
    "# df = pd.DataFrame.from_dict(dqn_noreb, orient='index').T\n",
    "\n",
    "# # Create a box plot using Plotly\n",
    "# fig = px.box(df, title=\"Box Plot of Data\")\n",
    "\n",
    "# # Show the plot\n",
    "# fig.show()\n",
    "# #%%\n",
    "# # Create a DataFrame from the dictionary, excluding keys with empty values\n",
    "# df = pd.DataFrame.from_dict(bm_noreb, orient='index').T\n",
    "\n",
    "# # Create a box plot using Plotly\n",
    "# fig = px.box(df, title=\"Box Plot of Data\")\n",
    "\n",
    "# # Show the plot\n",
    "# fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
